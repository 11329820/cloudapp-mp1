{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd3ff52",
   "metadata": {
    "_cell_guid": "68194ab7-bebb-4ea6-8cf3-3f0412fa1f16",
    "_uuid": "335c4aae-8f78-4ad3-b432-1f21cd307195",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-02T23:53:58.336295Z",
     "iopub.status.busy": "2025-09-02T23:53:58.335885Z",
     "iopub.status.idle": "2025-09-02T23:53:58.343665Z",
     "shell.execute_reply": "2025-09-02T23:53:58.342529Z",
     "shell.execute_reply.started": "2025-09-02T23:53:58.336269Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00139,
     "end_time": "2025-09-07T21:01:02.085142",
     "exception": false,
     "start_time": "2025-09-07T21:01:02.083752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# needed imports\n",
    "from transformers import AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch, langchain, faiss, chainlit, pymupdf, docutils\n",
    "#loading model\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    model_name = \"gpt2\"  # This is the \"path\" or identifier\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "# load encyclopedia\n",
    "doc = pymupdf.open(\"/storage/emulated/0/Download/The-Gale-Encyclopedia_medicaldiagnosis.pdf\")\n",
    "text = doc.load()\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512)\n",
    "chunks = text_splitter.split_text(text)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(chunks)\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "index.add(embeddings)\n",
    "#prompt engineering\n",
    "prompt_template = \"Q: {question}\\nA: {answer}\"\n",
    "from langchain.chains import RetrievalQAChain\n",
    "retrieval_qa = RetrievalQAChain(\n",
    "    llm=model,\n",
    "    vectorstore=index,\n",
    "    prompt_template=prompt_template\n",
    ")\n",
    "from chainlit import run\n",
    "run(retrieval_qa)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8192595,
     "sourceId": 12946056,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8213916,
     "sourceId": 12977417,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8221998,
     "sourceId": 12989754,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.434302,
   "end_time": "2025-09-07T21:01:02.303935",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-07T21:00:57.869633",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
